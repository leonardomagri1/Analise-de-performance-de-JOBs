{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 1. Setup GiftEval Benchmark\n",
        "\n",
        "1.1 To run the GiftEval Benchmark you need to download the data first (For further information see [the GiftEval repository](https://github.com/SalesforceAIResearch/gift-eval). \n",
        ")\n",
        "\n",
        "`huggingface-cli download Salesforce/GiftEval --repo-type=dataset --local-dir PATH_TO_SAVE`\n",
        "\n",
        "1.2 Additionally you need to install the dependencies in [./requirements.txt](requirements.txt)\n",
        "\n",
        "`pip install -r ./examples/gifteval/requirements.txt`\n",
        "\n",
        "1.3 Set the `GIFT_EVAL` env variable to the download directory of GiftEval (see below)"
      ]
    },
    {
      "metadata": {},
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"GIFT_EVAL\"] = \"path/to/gifteval\"  # You need to set the path to your GiftEval storage\n",
        "# os.environ[\"TIREX_NO_CUDA\"] = \"1\"  # Experimental!!: Turns off sLSTM CUDA kernels if you have problems but be aware of the downsides! (see repository FAQ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 2. Load Model and Run Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tirex import ForecastModel, load_model\n",
        "\n",
        "model: ForecastModel = load_model(\"NX-AI/TiRex\", device=\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from gift_eval_utils import TiRexGiftEvalWrapper, evaluate_dataset, gift_eval_dataset_iter\n",
        "\n",
        "wrapped_model = TiRexGiftEvalWrapper(model)\n",
        "results = []\n",
        "for task in gift_eval_dataset_iter():\n",
        "    task_result = evaluate_dataset(wrapped_model, **task)\n",
        "    results.append(task_result)\n",
        "    print(task_result)\n",
        "results = pd.DataFrame(results)\n",
        "print(results)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
